{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "83708667-4fdc-1563-7b3a-06b6575d2865",
        "id": "nbmWyf0Dsenu"
      },
      "source": [
        "# NLP-Master Template\n",
        "\n",
        "How do you work through various NLP related tasks using different Python packages for NLP ?\n",
        "\n",
        "In this jupyter notebook you will work through the above mentioned steps of Data-preprocessing and feature extraction using different libraries for NLP.\n",
        "\n",
        "There are many python packages for NLP out there, but we can cover the important bases once we master a handful of them. In this jupyter notebook we will describe following Python NLP libraries we’ve found to be the most useful and will be using in the case studies.\n",
        "\n",
        "   * NLTK Book: https://www.nltk.org/    \n",
        "   * TextBlob : https://textblob.readthedocs.io/en/dev/index.html    \n",
        "   * Spacy : https://spacy.io/\n",
        "\n",
        "In additional to these there are few other libraries such as Gensim and Stanford’s CoreNLP that can be explored as well.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF3M0M_jsenv"
      },
      "source": [
        "## Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QciHPhRZsenw"
      },
      "source": [
        "* [1. Loading Libraries and Packages](#1)\n",
        "* [2. Data Preprocessing](#2)\n",
        "    * [2.1. Tokenization](#2.1)    \n",
        "    * [2.2. Removing Stop Words](#2.2)\n",
        "    * [2.3. Stemming](#2.3)\n",
        "    * [2.4. Lemmetization](#2.4)\n",
        "    * [2.5. PoS tagging](#2.5)\n",
        "    * [2.6. Name Entity Recognition](#2.6)  \n",
        "* [3. Feature Representation](#3)\n",
        "    * [3.1. Bag-of Words](#3.1)    \n",
        "    * [3.2. TF-IDF](#3.2)\n",
        "    * [3.3. Word Embedding](#3.3)\n",
        "* [4. Inference](#4)\n",
        "    * [4.1. Supervised (Example Naive Bayes)](#4.1)    \n",
        "    * [4.2. Unsupervised (Example LDA)](#4.2)\n",
        "* [5. NLP Recipies](#5)\n",
        "    * [5.1. Sentiment Analysis](#5.1)\n",
        "    * [5.2. Words and Sentences similarity](#5.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT2YR6Uvsenw"
      },
      "source": [
        "<a id='1'></a>\n",
        "# 1. Load libraries and Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erWtctzpsenw"
      },
      "source": [
        "As a first step we check if the additional packages needed are present, if not install them. These are checked separately as they aren't included in requirement.txt as they aren't used for all case studies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBOU7qdTsenw",
        "outputId": "05c222b2-7eb2-4d03-bf43-04ba56faad78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2788561284.py:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  import pkg_resources\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All required packages are already installed.\n"
          ]
        }
      ],
      "source": [
        "import pkg_resources\n",
        "import pip\n",
        "import sys\n",
        "installedPackages = {pkg.key for pkg in pkg_resources.working_set}\n",
        "required = {'nltk', 'spacy', 'textblob','gensim' }\n",
        "missing = required - installedPackages\n",
        "if missing:\n",
        "    print(\"Installing missing packages...\")\n",
        "    if 'nltk' in missing:\n",
        "        !pip uninstall -y nltk\n",
        "        !pip install --no-cache-dir --upgrade nltk\n",
        "    if 'textblob' in missing:\n",
        "        !pip install --no-cache-dir --upgrade textblob\n",
        "    if 'gensim' in missing:\n",
        "        !pip install --no-cache-dir --upgrade gensim\n",
        "    if 'spacy' in missing:\n",
        "        !pip install --no-cache-dir --upgrade spacy\n",
        "        !python -m spacy download en_core_web_lg\n",
        "else:\n",
        "    print(\"All required packages are already installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y nltk\n",
        "!pip install nltk==3.8.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN__MV_ytkOt",
        "outputId": "59327000-89e5-47c2-e194-8478eb07ab96"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: nltk 3.8.1\n",
            "Uninstalling nltk-3.8.1:\n",
            "  Successfully uninstalled nltk-3.8.1\n",
            "Collecting nltk==3.8.1\n",
            "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk==3.8.1) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk==3.8.1) (4.67.1)\n",
            "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "Installing collected packages: nltk\n",
            "Successfully installed nltk-3.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPPsasA3senx"
      },
      "source": [
        "For NLTK, import NLTK and run nltk.download().This will open the NLTK downloader from where you can choose the corpora and models to download. You can also download all packages at once.\n",
        "\n",
        "For spacy, download spacy core model by running the command \"python -m spacy download en_core_web_sm\". Once downloaded, load this int he jupyter notebook by using spacy.load()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGMyDbZssenx",
        "outputId": "b94338fe-34d8-4552-ce8c-792f58c5ca94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from textblob import TextBlob\n",
        "import spacy\n",
        "#Run the command python -m spacy download en_core_web_sm to download this\n",
        "import en_core_web_lg\n",
        "nlp = en_core_web_lg.load()\n",
        "\n",
        "#Other helper packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Download nltk data lobraries. All can be downloaded by using nltk.download('all')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZN0jrzqxseny"
      },
      "outputs": [],
      "source": [
        "#Diable the warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7Phe4Slseny"
      },
      "source": [
        "<a id='2'></a>\n",
        "# 2. Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQhRcN3qseny"
      },
      "source": [
        "<a id='2.1'></a>\n",
        "## 2.1. Tokenization\n",
        "Tokenization is just the term used to describe the process of converting the normal text strings into a list of tokens i.e words that we actually want. Sentence tokenizer can be used to find the list of sentences and Word tokenizer can be used to find the list of words in strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "_cell_guid": "5d8fee34-f454-2642-8b06-ed719f0317e1",
        "id": "P8DLDTUPseny"
      },
      "outputs": [],
      "source": [
        "#Text to tokenize\n",
        "text = \"This is a tokenize test\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZUpezG6seny"
      },
      "source": [
        "### NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7luh0Z8seny"
      },
      "source": [
        "The NLTK data package includes a pre-trained Punkt tokenizer for English, which has alreayd been loaded before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKbpoOeMseny",
        "outputId": "719b3f14-5eea-4e42-d511-1492f40e330d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'a', 'tokenize', 'test']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vgdE1kIsenz"
      },
      "source": [
        "### TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcLvamyasenz",
        "outputId": "6964d15e-9df6-45e9-f8d3-184b31683fe7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['This', 'is', 'a', 'tokenize', 'test'])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "TextBlob(text).words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljOd-kVosenz"
      },
      "source": [
        "<a id='2.2'></a>\n",
        "## 2.2. Stop Words Removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfc2_x1Asenz"
      },
      "source": [
        "Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words. The code for removing stop words using SpaCy library is shown below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmgysUylsenz"
      },
      "source": [
        "### NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-VGl_sqsenz"
      },
      "source": [
        "We first load the language model and store it in the stop_words variable. The stopwords.words('english') is a set of default stop words for English language model in NLTK. Next, we simply iterate through each word in the input text and if the word exists in the stop word set of the NLTK language model, the word is removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pzbWWkNHsenz"
      },
      "outputs": [],
      "source": [
        "text = \"S&P and NASDAQ are the two most popular indices in US\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "204xE_mdsenz",
        "outputId": "033681b8-ba71-463b-82d5-f066379d136a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['S', '&', 'P', 'NASDAQ', 'two', 'popular', 'indices', 'US']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "text_tokens = word_tokenize(text)\n",
        "tokens_without_sw= [word for word in text_tokens if not word in stop_words]\n",
        "\n",
        "print(tokens_without_sw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0M7H_oBsenz"
      },
      "source": [
        "As we can see some of the stop words such as \"are\", \"of\", \"most\" etc are removed from the sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq3bCuufsenz"
      },
      "source": [
        "<a id='2.3'></a>\n",
        "## 2.3. Stemming\n",
        "Stemming is the process of reducing inflected (or sometimes derived) words to their stem, base or root form — generally a written word form. Example if we were to stem the following words: “Stems”, “Stemming”, “Stemmed”, “and Stemtization”, the result would be a single word “stem”."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jDJeBWqXsenz"
      },
      "outputs": [],
      "source": [
        "text = \"It's a Stemming testing\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKtGrffusenz"
      },
      "source": [
        "### NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mlJHzx7gsenz"
      },
      "outputs": [],
      "source": [
        "parsed_text = word_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dkb9R_0lsenz",
        "outputId": "ecf19eab-b0dc-46d4-fa3e-52963c6aea38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Stemming', 'stem'), ('testing', 'test')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Initialize stemmer.\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "# Stem each word.\n",
        "[(word, stemmer.stem(word)) for i, word in enumerate(parsed_text)\n",
        " if word.lower() != stemmer.stem(parsed_text[i])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxHty_jpsen0"
      },
      "source": [
        "<a id='2.4'></a>\n",
        "## 2.4. Lemmetization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyJnAqfesen0"
      },
      "source": [
        "A slight variant of stemming is lemmatization. The major difference between these is, that, stemming can often create non-existent words, whereas lemmas are actual words. So, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary, but you can look up a lemma. Examples of Lemmatization are that “run” is a base form for words like “running” or “ran” or that the word “better” and “good” are in the same lemma so they are considered the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlqrCTBHsen0"
      },
      "source": [
        "### TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PbFdTIRFsen0"
      },
      "outputs": [],
      "source": [
        "text = \"This world has a lot of faces \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kI6EXH66sen0",
        "outputId": "b13f1a96-6ee2-4928-cc06-548fc4145692"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['This', 'world', 'has', 'a', 'lot', 'of', 'faces'])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from textblob import Word\n",
        "parsed_data= TextBlob(text).words\n",
        "parsed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvcB5Wodsen0",
        "outputId": "7592efc1-70d0-4ea3-d749-bcdb03455b08"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('has', 'ha'), ('faces', 'face')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "[(word, word.lemmatize()) for i, word in enumerate(parsed_data)\n",
        " if word != parsed_data[i].lemmatize()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7tT86IRsen0"
      },
      "source": [
        "<a id='2.5'></a>\n",
        "## 2.5. POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX-T3k2Ksen0"
      },
      "source": [
        "Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_9OACdcVsen0"
      },
      "outputs": [],
      "source": [
        "text = 'Google is looking at buying U.K. startup for $1 billion'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjJpH73wsen0"
      },
      "source": [
        "### TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiVe7pfxsen3",
        "outputId": "8947984a-06b5-41f3-ce86-8f434f86a5bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Google', 'NNP'),\n",
              " ('is', 'VBZ'),\n",
              " ('looking', 'VBG'),\n",
              " ('at', 'IN'),\n",
              " ('buying', 'VBG'),\n",
              " ('U.K.', 'NNP'),\n",
              " ('startup', 'NN'),\n",
              " ('for', 'IN'),\n",
              " ('1', 'CD'),\n",
              " ('billion', 'CD')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "TextBlob(text).tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va7dxWw0sen4"
      },
      "source": [
        "## Spacy- doing all at ones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-Cc-4Xusen4"
      },
      "source": [
        "When you call nlp on a text, spaCy first tokenizes the text to produce a Doc object. The Doc is then processed in several different steps – this is also referred to as the processing pipeline. The pipeline used by the default models consists of a tagger, a parser and an entity recognizer. Each pipeline component returns the processed Doc, which is then passed on to the next component.\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "All the preprocessing items including tokenization, stop words removal, lemmatization, getting POS and NER etc. can be performed in one go using spaCy. An example is demonstrated below. We will go through the example of NER in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "yUrM_P9Gsen4"
      },
      "outputs": [],
      "source": [
        "text = 'Google is looking at buying U.K. startup for $1 billion'\n",
        "doc = nlp(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "MMmaxS4Gsen4",
        "outputId": "30a4b7ab-6391-4070-a943-11aaf55f5502"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Token  is_stop_word    lemma    POS\n",
              "0    Google         False   Google  PROPN\n",
              "1        is          True       be    AUX\n",
              "2   looking         False     look   VERB\n",
              "3        at          True       at    ADP\n",
              "4    buying         False      buy   VERB\n",
              "5      U.K.         False     U.K.  PROPN\n",
              "6   startup         False  startup   VERB\n",
              "7       for          True      for    ADP\n",
              "8         $         False        $    SYM\n",
              "9         1         False        1    NUM\n",
              "10  billion         False  billion    NUM"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-95a46984-9b79-4f13-81fe-a8e2cde8ad97\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Token</th>\n",
              "      <th>is_stop_word</th>\n",
              "      <th>lemma</th>\n",
              "      <th>POS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Google</td>\n",
              "      <td>False</td>\n",
              "      <td>Google</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is</td>\n",
              "      <td>True</td>\n",
              "      <td>be</td>\n",
              "      <td>AUX</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>looking</td>\n",
              "      <td>False</td>\n",
              "      <td>look</td>\n",
              "      <td>VERB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>at</td>\n",
              "      <td>True</td>\n",
              "      <td>at</td>\n",
              "      <td>ADP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>buying</td>\n",
              "      <td>False</td>\n",
              "      <td>buy</td>\n",
              "      <td>VERB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>U.K.</td>\n",
              "      <td>False</td>\n",
              "      <td>U.K.</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>startup</td>\n",
              "      <td>False</td>\n",
              "      <td>startup</td>\n",
              "      <td>VERB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>for</td>\n",
              "      <td>True</td>\n",
              "      <td>for</td>\n",
              "      <td>ADP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>$</td>\n",
              "      <td>False</td>\n",
              "      <td>$</td>\n",
              "      <td>SYM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>NUM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>billion</td>\n",
              "      <td>False</td>\n",
              "      <td>billion</td>\n",
              "      <td>NUM</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-95a46984-9b79-4f13-81fe-a8e2cde8ad97')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-95a46984-9b79-4f13-81fe-a8e2cde8ad97 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-95a46984-9b79-4f13-81fe-a8e2cde8ad97');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-1d605bb4-a398-4f4d-bef2-a62c42c82232\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1d605bb4-a398-4f4d-bef2-a62c42c82232')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-1d605bb4-a398-4f4d-bef2-a62c42c82232 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"             columns=['Token', 'is_stop_word','lemma', 'POS'])\",\n  \"rows\": 11,\n  \"fields\": [\n    {\n      \"column\": \"Token\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \"U.K.\",\n          \"Google\",\n          \"1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_stop_word\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lemma\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \"U.K.\",\n          \"Google\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"POS\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"PROPN\",\n          \"AUX\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "pd.DataFrame([[t.text, t.is_stop, t.lemma_, t.pos_]\n",
        "              for t in doc],\n",
        "             columns=['Token', 'is_stop_word','lemma', 'POS'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLQtdE1Asen4"
      },
      "source": [
        "spaCy also performs NER that we will discuss in the next section, along with the word embedding which we will also cover in the next section. Given NER performs a wide range of NLP related tasks in one go, it is highly recommended. We will be using spaCy extensively in our case studies. The list of all the task that can be performed using spaCy is mentioned in the list below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-TiyR1Xsen4",
        "outputId": "6de94c7b-a616-4bf9-e918-286fa2b1419c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cats', 'char_span', 'copy', 'count_by', 'doc', 'ents', 'extend_tensor', 'from_array', 'from_bytes', 'from_dict', 'from_disk', 'from_docs', 'from_json', 'get_extension', 'get_lca_matrix', 'has_annotation', 'has_extension', 'has_unknown_spaces', 'has_vector', 'is_nered', 'is_parsed', 'is_sentenced', 'is_tagged', 'lang', 'lang_', 'mem', 'noun_chunks', 'noun_chunks_iterator', 'remove_extension', 'retokenize', 'sentiment', 'sents', 'set_ents', 'set_extension', 'similarity', 'spans', 'tensor', 'text', 'text_with_ws', 'to_array', 'to_bytes', 'to_dict', 'to_disk', 'to_json', 'to_utf8_array', 'user_data', 'user_hooks', 'user_span_hooks', 'user_token_hooks', 'vector', 'vector_norm', 'vocab']\n"
          ]
        }
      ],
      "source": [
        "attributes = [a for a in dir(doc) if not a.startswith('_')]\n",
        "print(attributes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk5Dw7hbsen4"
      },
      "source": [
        "<a id='2.6'></a>\n",
        "## 2.6. Name Entity Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKqI9cFWsen4"
      },
      "source": [
        "Named Entity Recognition, popularly referred to as N.E.R is a process that seeks to locate and classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc. NER is used in many fields in Natural Language Processing (NLP), and it can help answering many real-world questions. The NER performed using spaCy is shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qHXuYb1bsen4"
      },
      "outputs": [],
      "source": [
        "text = 'Google is looking at buying U.K. startup for $1 billion'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekSli5w6sen4"
      },
      "source": [
        "### SpaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgsLnr_9sen4",
        "outputId": "5038358d-31c4-44f2-dc4a-8f673e24001f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity:  Google\n",
            "Entity Type: ORG | Companies, agencies, institutions, etc.\n",
            "--\n",
            "Entity:  U.K.\n",
            "Entity Type: GPE | Countries, cities, states\n",
            "--\n",
            "Entity:  $1 billion\n",
            "Entity Type: MONEY | Monetary values, including unit\n",
            "--\n"
          ]
        }
      ],
      "source": [
        "for entity in nlp(text).ents:\n",
        "    print(\"Entity: \", entity.text)\n",
        "    print(\"Entity Type: %s | %s\" % (entity.label_, spacy.explain(entity.label_)))\n",
        "    print(\"--\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "YTZKs_fWsen4",
        "outputId": "8c42eae6-3bf8-4f30-b7bc-6a065e55d725"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Google\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is looking at buying \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    U.K.\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " startup for \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    $1 billion\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              "</div></span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from spacy import displacy\n",
        "displacy.render(nlp(text), style=\"ent\", jupyter = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "df6a4523-b385-69ee-c933-592826d81431",
        "id": "gKg4RVilsen4"
      },
      "source": [
        "<a id='3'></a>\n",
        "# 3. Feature Representation\n",
        "\n",
        "The vast majority of NLP related data is created for human consumption and as such is stored\n",
        "in an unstructured format, such as news feed articles, PDF reports, social media posts\n",
        "and audio files, which cannot be readily processed by computers. Following the preprocessing steps discussed in the previous section, in order for the information content to be conveyed to the statistical inference algorithm, the preprocessed tokens need to be translated into predictive features. A model is used to embed raw text into a vector space where we can use the data science tool.\n",
        "\n",
        "Feature representation involves two things:\n",
        "* A vocabulary of known words.\n",
        "* A measure of the presence of known words.\n",
        "\n",
        "The intuition behind the Feature Representation is that documents are similar if they have similar content. Also, we can learn something about the meaning of the document from its content alone.\n",
        "For example, if our dictionary contains the words {Learning, is, the, not, great}, and we want to vectorize the text “Learning is great”, we would have the following vector: (1, 1, 0, 0, 1).\n",
        "\n",
        "Some of the feature representation methods are as follows:\n",
        "* Bag of Words- word count\n",
        "* Tf-Idf\n",
        "* Word Embedding\n",
        "    * Pretrained word embedding models ( Word2vec, GloVe)\n",
        "    * Customized deep Learning based\n",
        "\n",
        "There are Feature representation(or vector representation) such as one-hot encoding of text, n-grams etc which are similar to the types mentioned above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5CMlu6Psen5"
      },
      "source": [
        "<a id='3.1'></a>\n",
        "## 3.1. Bag of Words - Word Count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMKaNUyMsen5"
      },
      "source": [
        "In natural language processing, a common technique for extracting features from text is to place all of the words that occur in the text in a bucket. This aproach is called a bag of words model or BoW for short. It’s referred to as a “bag” of words because any information about the structure of the sentence is lost.The CountVectorizer from sklearn provides a simple way to both tokenize a collection of text documents and encode new documents using that vocabulary.The fit_transform\n",
        "function learns the vocabulary from one or more documents and encodes each document in the word as a vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "NC5Gjmz3sen5"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "'The stock price of google jumps on the earning data today',\n",
        "'Google plunge on China Data!'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnX_G8odsen5",
        "outputId": "5838ffe9-95c4-4c26-d71c-55d4e1d204bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 1 1 1 1 1 0 1 1 2 1]\n",
            " [1 1 0 1 0 0 1 1 0 0 0 0]]\n",
            "{'the': 10, 'stock': 9, 'price': 8, 'of': 5, 'google': 3, 'jumps': 4, 'on': 6, 'earning': 2, 'data': 1, 'today': 11, 'plunge': 7, 'china': 0}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "print( vectorizer.fit_transform(sentences).todense() )\n",
        "print( vectorizer.vocabulary_ )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NkkkRwHsen5"
      },
      "source": [
        "We can see an array version of the encoded vector showing a count of 1 occurrence for each word except the (index and id 10) that has an occurrence of 2. Word counts are a good starting point, but are very basic.One issue with simple counts is that some words like “the” will appear many times and their large counts will not be very meaningful in the encoded vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6XWJoMDsen5"
      },
      "source": [
        "<a id='3.2'></a>\n",
        "## 3.2. TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaryKFuasen5"
      },
      "source": [
        "An alternative is to calculate word frequencies, and by far the most popular method is called TF-IDF. This is an acronym than stands for “Term Frequency – Inverse Document” Frequency which are the components of the resulting scores assigned to each word.\n",
        "\n",
        "* Term Frequency: This summarizes how often a given word appears within a document.\n",
        "* Inverse Document Frequency: This downscales words that appear a lot across documents.\n",
        "Without going into the math, TF-IDF are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents.\n",
        "\n",
        "The TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waqqG_NAsen5",
        "outputId": "6b930b2e-1f83-4f3e-9208-b8235f3c3871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['china' 'data' 'earning' 'google' 'jumps' 'plunge' 'price' 'stock'\n",
            " 'today']\n",
            "(2, 9)\n",
            "[[0.         0.29017021 0.4078241  0.29017021 0.4078241  0.\n",
            "  0.4078241  0.4078241  0.4078241 ]\n",
            " [0.57615236 0.40993715 0.         0.40993715 0.         0.57615236\n",
            "  0.         0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "TFIDF = vectorizer.fit_transform(sentences)\n",
        "print(vectorizer.get_feature_names_out()[-10:])\n",
        "print(TFIDF.shape)\n",
        "print(TFIDF.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ2yDtaxsen5"
      },
      "source": [
        "A vocabulary of 9 words is learned from the documents and each word is assigned a unique integer index in the output vector. The sentences are encoded as an 9-element sparse array and we can review the final scorings of each word with different values from the other words in the vocabulary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9jJOFdssen5"
      },
      "source": [
        "<a id='3.3'></a>\n",
        "## 3.3. Word Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz6_LwCBsen6"
      },
      "source": [
        "A word embedding is a class of approaches for representing words and documents using a dense vector representation.\n",
        "\n",
        "It is an improvement over more the traditional bag-of-word model encoding schemes where large sparse vectors were used to represent each word or to score each word within a vector to represent an entire vocabulary. These representations were sparse because the vocabularies were vast and a given word or document would be represented by a large vector comprised mostly of zero values.\n",
        "\n",
        "Instead, in an embedding, words are represented by dense vectors where a vector represents the projection of the word into a continuous vector space.The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used. The position of a word in the learned vector space is referred to as its embedding.\n",
        "\n",
        "Two popular examples of methods of learning word embeddings from text include:\n",
        "* Pretained models( i.e. Word2Vec, glove etc.)\n",
        "* Developing custom models\n",
        "\n",
        "In addition to these carefully designed methods, a word embedding can be learned as part of a deep learning model. This can be a slower approach, but tailors the model to a specific training dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nfMIPRVsen6"
      },
      "source": [
        "### 3.3.1 Pretrained word embedding models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46CGNoLIsen6"
      },
      "source": [
        "### 3.3.1.1  Pretrained model- SpaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN9jajWgsen6"
      },
      "source": [
        "SpaCy comes with inbuilt representation of text as vectors at different levels of word, sentence and document. The underlying vector representations come from a word embedding model which generally produces a dense multi-dimensional semantic representation of words (as shown in the example below). The word embedding model includes 20k unique vectors with 300 dimensions. Using this vector representation, we can calculate similarities and dissimilarities between tokens, named entities, noun phrases, sentences and documents.\n",
        "\n",
        "The word embedding in Spacy is performed first by first loading the model, and then processing text. The vectors can be accessed directly using the .vector attribute of each processed token (word). The mean vector for the entire sentence is also calculated simply using .vector, providing a very convenient input for machine learning models based on sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "B5cu6Flnsen6"
      },
      "outputs": [],
      "source": [
        "doc = nlp(\"Apple orange cats dogs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WC6KmaYsen6",
        "outputId": "cbf7a7b8-ef1b-43b7-9ba2-62387c282762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector representation of the sentence for first 10 features: \n",
            " [-0.30732775  0.22351399 -0.110111   -0.367025   -0.13430001  0.13790375\n",
            " -0.24379876 -0.10736975  0.2715925   1.3117325 ]\n"
          ]
        }
      ],
      "source": [
        "print(\"Vector representation of the sentence for first 10 features: \\n\", doc.vector[0:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JesT-3zrsen6"
      },
      "source": [
        "### 3.3.1.2. Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "e-7DDUCosen6"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOZsDfUbsen6",
        "outputId": "cf991857-203b-4464-fc3c-ee64a2c951ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec<vocab=10, vector_size=100, alpha=0.025>\n",
            "['Google', ' Data!', 'China', ' on', 'plunge', 'increases', 'of', 'price', 'stock', 'The']\n",
            "[ 0.00023643  0.00510335  0.00900927 -0.00930295]\n"
          ]
        }
      ],
      "source": [
        "sentences = [\n",
        "['The','stock','price', 'of', 'Google', 'increases'],\n",
        "['Google','plunge',' on','China',' Data!']]\n",
        "# train model\n",
        "model = Word2Vec(sentences, min_count=1)\n",
        "# summarize the loaded model\n",
        "print(model)\n",
        "words = list(model.wv.key_to_index.keys())\n",
        "print(words)\n",
        "print(model.wv['Google'][1:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA_1VCyVsen6"
      },
      "source": [
        "<a id='4'></a>\n",
        "# 4. Interpretation\n",
        "Like all other artificial intelligence tasks, the inference generated by an NLP application\n",
        "usually needs to be translated into a decision in order to be actionable.Inference in ML falls under three broad categories, namely supervised, unsupervised and reinforcement learning. While the type of inference required depends on the business problem and the type of training data, in NLP the most commonly used algorithms are\n",
        "supervised or unsupervised.\n",
        "\n",
        "In the past years, neural network architectures, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs), have dominated NLP-based inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BymCTT5ksen6"
      },
      "source": [
        "<a id='4.1'></a>\n",
        "## 4.1. Supervised Learning Example-Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X939skasen6"
      },
      "source": [
        "One of the most commonly used supervised methodologies in NLP is the Naïve\n",
        "Bayes model, which assumes that all word features are independent of each other given\n",
        "the class labels. Due to this simplifying assumptions, Naïve Bayes is very compatible with a bag-of-words word representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "do9F-w7Dsen6"
      },
      "outputs": [],
      "source": [
        "senteces = [\n",
        "'The stock price of google jumps on the earning data today',\n",
        "'Google plunge on China Data!']\n",
        "sentiment = (1, 0)\n",
        "data = pd.DataFrame({'Sentence':senteces,\n",
        "        'sentiment':sentiment})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "YrwUygh3sen6"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vect = CountVectorizer().fit(data['Sentence'])\n",
        "X_train_vectorized = vect.transform(data['Sentence'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPsejbLOsen6",
        "outputId": "d2136e3b-4fb7-494c-96fb-e904aa45fecb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clfrNB = MultinomialNB(alpha = 0.1)\n",
        "clfrNB.fit(X_train_vectorized, data['sentiment'])\n",
        "\n",
        "preds = clfrNB.predict(vect.transform(['Apple price plunge', 'Amazon Price jumps']))\n",
        "preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDQQG5Q9sen6"
      },
      "source": [
        "As we can see that the Naive Bayes trains the model fairly well from the two sentences. The model gives a sentiment of 0 for the sentence \"Apple price plunge\" and 1 for the sentence \"Amazon Price jumps\", given the sentence used for training also had keywords \"plunge\" and \"jumps\" as were assigned to sentiments of 0 and 1 respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cLw-OAMsen7"
      },
      "source": [
        "<a id='4.2'></a>\n",
        "## 4.2. Unsupervised Learning Example-LDA\n",
        "LDA model is the most popular topic model because it tends to produce meaningful topics that\n",
        "humans can relate to, can assign topics to new documents, and is extensible. Variants of\n",
        "LDA models can include metadata such as authors, or image data, or learn hierarchical\n",
        "topics\n",
        "Given a set of documents, assume that there are some latent topics of documents that are not observed. Each document has a distribution over these topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "FUq0p4tQsen7"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "'The stock price of google jumps on the earning data today',\n",
        "'Google plunge on China Data!'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Mys42e2sen7",
        "outputId": "c1bc8b97-f719-47ad-cb84-a8c546499ecf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.91377772, 0.04311114, 0.04311114],\n",
              "       [0.86261361, 0.06869319, 0.06869319]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "#Getting the bag-of words\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "vect=CountVectorizer(ngram_range=(1,1),stop_words='english')\n",
        "sentences_vec=vect.fit_transform(sentences)\n",
        "\n",
        "#Running LDA on the bag of words.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "lda=LatentDirichletAllocation(n_components=3)\n",
        "lda.fit_transform(sentences_vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riupeR4Bsen7"
      },
      "source": [
        "The model produces two smaller matrices. We will be discussing the interpretation further in the third case study."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfF-kFORsen7"
      },
      "source": [
        "<a id='5'></a>\n",
        "# 5 NLP Recipies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82c0KWkLsen7"
      },
      "source": [
        "<a id='5.1'></a>\n",
        "## 5.1. Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68mLkYIcsen7"
      },
      "source": [
        "Sentiment analysis is contextual mining of text which identifies and extracts subjective information in source material, and helping us understand the sentiments behind a text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_VTUVY3sen7"
      },
      "source": [
        "With the help of Sentiment Analysis using Textblob the sentiment analysis can be performed in few lines of code. TextBlob provides polarity and subjectivity estimates for parsed documents using dictionaries provided by the Pattern library. The polarity defines the phase of emotions expressed in the analyzed sentence. Polarity alone is not enough to deal with complex text sentences. Subjectivity helps in determining personal states of the speaker including Emotions, Beliefs and opinions. It has values from 0 to 1 and a value closer to 0 shows the sentence is objective and vice versa.\n",
        "\n",
        "The texblob sentiment function is pretrained and map adjectives frequently found in movie reviews(source code: https://textblob.readthedocs.io/en/dev/_modules/textblob/en/sentiments.html) to sentiment polarity scores, ranging from -1 to +1 (negative ↔ positive) and a similar subjectivity score (objective ↔ subjective).\n",
        "\n",
        "The .sentiment attribute provides the average for each over the relevant tokens, whereas the .sentiment_assessments attribute lists the underlying values for each token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "zKzvaYH1sen7"
      },
      "outputs": [],
      "source": [
        "text1 = \"Bayer (OTCPK:BAYRY) started the week up 3.5% to €74/share in Frankfurt, touching their highest level in 14 months, after the U.S. government said a $25M glyphosate decision against the company should be reversed.\"\n",
        "text2 = \"Apple declares poor in revenues\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlhaVrTDsen7",
        "outputId": "fbfd26d3-fe62-471d-cd0a-5c3afd7258f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "TextBlob(text1).sentiment.polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ggzRLgxsen7",
        "outputId": "9e3347a3-d2ff-4297-fd2a-3c8e4db52e84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(polarity=0.5, subjectivity=0.5, assessments=[(['touching'], 0.5, 0.5, None)])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "TextBlob(text1).sentiment_assessments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMdyG1Q2sen7",
        "outputId": "070eb75d-1313-40ad-c9b2-3fb65386bd59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.4"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "TextBlob(text2).sentiment.polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKqyUbZMsen7",
        "outputId": "42e76e93-4e24-4f55-9a33-30f435b1c404"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(polarity=-0.4, subjectivity=0.6, assessments=[(['poor'], -0.4, 0.6, None)])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "TextBlob(text2).sentiment_assessments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxY8qQ0Isen7"
      },
      "source": [
        "We see that the first sentiment has positive sentiment and the second text has negative sentiments. Looking at the subjectivity, the second sentence has more subjectivity as compared to the first one. However, looking at the words that give rise to the sentiments, the word \"touching\" and not \"high\" causes positive sentiment in sentence one. So, probably a sentiment analysis algorithm pretrained on movie/product reviews might not perform well with news sentiment analysis. Hence probably, additional training for the stock sentiments might be needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fDBgDhysen7"
      },
      "source": [
        "<a id='5.2'></a>\n",
        "## 5.2. Text Similarity\n",
        "Finding similarity between text is at the heart of almost all text mining methods, for example, text classification, clustering, recommendation, and many more. In order to calculate similarity between two text snippets, the usual way is to convert the text into its corresponding vector representation, for which there are many methods like word embedding of text, and then calculate similarity or difference using different distance metrics such as cosine-similarity and euclidean distance applicable to vectors. The underlying vector representations come from a word embedding model which generally produces a dense multi-dimensional semantic representation of words (as shown in the example). Using this vector representation, we can calculate similarities and dissimilarities between tokens, named entities, noun phrases, sentences and documents. The example below shows how to calculate similarities between two documents and tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "BuHJ6IlYsen7"
      },
      "outputs": [],
      "source": [
        "text1 = \"Barack Obama was the 44th president of the United States of America.\"\n",
        "text2 = \"Donald Trump is the 45th president of the United States of America.\"\n",
        "text3 = \"SpaCy and NLTK are two popular NLP libraries in Python community.\"\n",
        "doc1 = nlp(text1); doc2 = nlp(text2); doc3 = nlp(text3);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "IXlihjalsen7"
      },
      "outputs": [],
      "source": [
        "def text_similarity(inp_obj1, inp_obj2):\n",
        "    return inp_obj1.similarity(inp_obj2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHeSq_oNsen7",
        "outputId": "fbd48bd3-ea5a-4033-ba4d-696621231f4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between doc1 and doc2:  0.9525885576862079\n",
            "Similarity between doc1 and doc3:  0.5184866919221098\n"
          ]
        }
      ],
      "source": [
        "print(\"Similarity between doc1 and doc2: \", text_similarity(doc1, doc2))\n",
        "print(\"Similarity between doc1 and doc3: \", text_similarity(doc1, doc3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeQanKoLsen7",
        "outputId": "36b64a67-cffc-49cc-b0ce-909e85e2ff38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token 1: Apple, Token 2: Apple - Similarity: 1.000000\n",
            "Token 1: Apple, Token 2: orange - Similarity: 0.561892\n",
            "Token 1: Apple, Token 2: cats - Similarity: 0.218511\n",
            "Token 1: orange, Token 2: Apple - Similarity: 0.561892\n",
            "Token 1: orange, Token 2: orange - Similarity: 1.000000\n",
            "Token 1: orange, Token 2: cats - Similarity: 0.267099\n",
            "Token 1: cats, Token 2: Apple - Similarity: 0.218511\n",
            "Token 1: cats, Token 2: orange - Similarity: 0.267099\n",
            "Token 1: cats, Token 2: cats - Similarity: 1.000000\n"
          ]
        }
      ],
      "source": [
        "def token_similarity(doc):\n",
        "    for token1 in doc:\n",
        "        for token2 in doc:\n",
        "            print(\"Token 1: %s, Token 2: %s - Similarity: %f\" % (token1.text, token2.text, token1.similarity(token2)))\n",
        "\n",
        "doc4 = nlp(\"Apple orange cats\")\n",
        "token_similarity(doc4)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 206,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}